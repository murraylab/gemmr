"""Functionality to estimate CCA/PLS models and analyze outcomes."""

import numbers
import warnings
from datetime import datetime

import numpy as np
import xarray as xr
import pandas as pd
from scipy.stats import pearsonr
from scipy.spatial.distance import cdist
from sklearn.utils import check_random_state

from tqdm.autonotebook import tqdm, trange

from joblib import Parallel, delayed

from ..generative_model import GEMMR, generate_data
from ..util import align_weights, _calc_true_loadings
from ..estimators import SVDPLS, SVDCCA
from ..estimators.helpers import _calc_cov, _calc_corr, SingularMatrixError

try:
    from ..estimators import SparsePLS
except ImportError:
    pass


__all__ = [
    'analyze_dataset', 'analyze_resampled', 'analyze_subsampled',
    'analyze_splithalf', 'analyze_model_light', 'analyze_model_parameters',
    'analyze_model'
]


class JointCovarianceWarning(Warning):
    pass


warnings.simplefilter('always', JointCovarianceWarning)


def analyze_dataset(estr, X, Y, Xorig=None, Yorig=None,
                    x_align_ref=None, y_align_ref=None,
                    addons=tuple(), fit_params=None, **kwargs
                    ):
    """Analyze a given dataset with a given estimator

    Fits an estimator to a given dataset, stores association strengths, weights
    and loadings and optionally runs additional analyses with fitted estimator.

    Parameters
    ----------
    estr : **sklearn**-style estimator
        for performing CCA or PLS. Must have method ``fit`` and (after fitting)
        attributes ``assocs_``, ``x_rotations_``, ``y_rotations_``,
        ``x_scores_``, ``y_scores_``
    X : np.ndarray (n_samples, n_features)
        dataset `X`
    Y : np.ndarray (n_samples, n_features)
        dataset `Y`
    Xorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `X`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``X`` and ``Xorig`` correspond to the same samples
        (subjects).
    Yorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `Y`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``Y`` and ``Yorig`` correspond to the same samples
        (subjects).
    x_align_ref : (n_features,)
        after fitting, the sign of `X` weights is chosen such that the
        cosine-distance between fitted `X` weights and ``x_align_ref`` is
        positive
    y_align_ref : (n_features,)
        after fitting, the sign of `Y` weights is chosen such that the
        cosine-distance between fitted `Y` weights and ``y_align_ref`` is
        positive
    addons : list-like of add-on functions
        After fitting the estimator and saving association strengths, weights
        and loadings in ``results`` additional analyses can be performed with
        these functions. They are called in the given order, and must have the
        signature

        .. code-block:: python

            addana_fun(estr, X, Y, Xorig, Yorig, x_align_ref, y_align_ref,
                results, **kwargs)

        and are expected to save their respective outcome features ``results``.
        Various such functions are provided in module
        :mod:`sample_analysis_addons`
    fit_params : dict
        keyword-arguments for estr.fit

    kwargs : dict
        forwarded to additional analysis functions

    Returns
    -------
    results : xr.Dataset
        containing data variables for outcome features generated by analyses
    """

    if fit_params is None:
        fit_params = dict()

    px = X.shape[1]
    py = Y.shape[1]

    # x_covs = np.cov(X.T)
    # y_covs = np.cov(Y.T)
    # assert(x_covs.shape == (px, px))

    try:
        estr.fit(X, Y, **fit_params)

    except (ValueError, SingularMatrixError) as e:
        warnings.warn('Fitting error: {}'.format(e))
        between_assocs = np.nan
        between_covs_sample = np.nan
        between_corrs_sample = np.nan
        x_weights = x_loadings = np.nan * np.empty((X.shape[1], 1))
        y_weights = y_loadings = np.nan * np.empty((Y.shape[1], 1))

        # code below references x/y_orig_loadings
        x_orig_loadings = None
        y_orig_loadings = None

    else:  # fit successful

        between_assocs = estr.assocs_[0]  # estr.corrs_[0]#estr.s_[0]

        if np.isfinite(estr.x_scores_[:, 0]).all() and \
                np.isfinite(estr.y_scores_[:, 0]).all():
            between_covs_sample = _calc_cov(
                estr.x_scores_[:, 0], estr.y_scores_[:, 0])
            between_corrs_sample = _calc_corr(
                estr.x_scores_[:, 0], estr.y_scores_[:, 0])
        else:
            between_covs_sample = np.nan
            between_corrs_sample = np.nan

        x_weights = estr.x_rotations_.copy()
        y_weights = estr.y_rotations_.copy()

        # x_loadings = np.dot(Xorig_z_.T, zscore(estr.x_scores_[:, 0])) / (n)
        # y_loadings = np.dot(Yorig_z_.T,
        #   zscore(estr.y_scores_[Ymask, 0])) / (n_Ymask)
        try:
            x_loadings = estr.x_loadings_
            y_loadings = estr.y_loadings_
        except AttributeError:
            x_loadings = _calc_loadings(X, estr.x_scores_[:, [0]])
            y_loadings = _calc_loadings(Y, estr.y_scores_[:, [0]])
        if Xorig is not None:
            x_orig_loadings = _calc_loadings(Xorig, estr.x_scores_[:, [0]])
        else:
            x_orig_loadings = None
        if Yorig is not None:
            y_orig_loadings = _calc_loadings(Yorig, estr.y_scores_[:, [0]])
        else:
            y_orig_loadings = None

        # align
        for mode in range(x_weights.shape[1]):
            if x_align_ref is not None:
                _x_weights, signs = align_weights(
                    x_weights[:, mode], x_align_ref[:, mode], return_sign=True)
                assert signs.shape == (1, 1)
                x_weights[:, mode] = _x_weights[0]
                x_loadings[:, mode] *= signs[0, 0]
                if x_orig_loadings is not None:
                    x_orig_loadings[:, mode] *= signs[0, 0]
                # CAREFUL: x_weights needs to be a COPY of x_rotations_,
                # otherwise "*= signs[0, 0]" will reverse the sign
                # modification!!!
                estr.x_rotations_[:, mode] *= signs[0, 0]

        for mode in range(y_weights.shape[1]):
            if y_align_ref is not None:
                _y_weights, signs = align_weights(
                    y_weights[:, mode], y_align_ref[:, mode], return_sign=True)
                assert signs.shape == (1, 1)
                y_weights[:, mode] = _y_weights[0]
                y_loadings[:, mode] *= signs[0, 0]
                if y_orig_loadings is not None:
                    y_orig_loadings[:, mode] *= signs[0, 0]
                # CAREFUL: y_weights needs to be a COPY of y_rotations_,
                # otherwise "*= signs[0, 0]" will reverse the sign
                # modification!!!
                estr.y_rotations_[:, mode] *= signs[0, 0]

    x_coords = dict(x_feature=np.arange(px))
    y_coords = dict(y_feature=np.arange(py))
    results = xr.Dataset(dict(
        between_assocs=between_assocs,
        between_covs_sample=between_covs_sample,
        between_corrs_sample=between_corrs_sample,
        x_weights=xr.DataArray(x_weights,
                               dims=('x_feature', 'mode'), coords=x_coords),
        y_weights=xr.DataArray(y_weights,
                               dims=('y_feature', 'mode'), coords=y_coords),
        x_loadings=xr.DataArray(x_loadings, dims=('x_feature', 'mode'),
                                coords=dict(
                                    x_feature=np.arange(X.shape[1]))),
        y_loadings=xr.DataArray(y_loadings, dims=('y_feature', 'mode'),
                                coords=dict(
                                    y_feature=np.arange(Y.shape[1]))),
    ))
    if x_orig_loadings is not None:
        results['x_orig_loadings'] = xr.DataArray(
            x_orig_loadings, dims=('x_orig_feature', 'mode'),
            coords=dict(x_orig_feature=np.arange(Xorig.shape[1]))
        )
    if y_orig_loadings is not None:
        results['y_orig_loadings'] = xr.DataArray(
            y_orig_loadings, dims=('y_orig_feature', 'mode'),
            coords=dict(y_orig_feature=np.arange(Yorig.shape[1]))
        )

    for ana_fun in addons:
        ana_fun(estr, X, Y, Xorig, Yorig, x_align_ref, y_align_ref, results,
                fit_params=fit_params, **kwargs)

    return results


def _calc_loadings(X, scores):
    """Calculate loadings, i.e. Pearson correlations between features and
    scores.

    If any of the entries in X is np.nan, loadings will be calculated from
    finite samples (rows) only.

    Parameters
    ----------
    X : np.ndarray (n_samples, n_features)
        a dataset containing features in columns for which to calculate
        loadings
    scores : np.ndarray (n_samples, n_modes)
        scores used to calculate loadings. Samples in rows must correspond to
        samples in rows of ``X``

    Returns
    -------
    loadings : np.ndarray (n_features, n_modes)
        calculated loadings
    """
    finite_X = np.isfinite(X)
    if finite_X.all():  # not np.ma.isMaskedArray(X):
        loadings = 1 - cdist(X.T, scores.T, metric='correlation')
    else:
        loadings = np.nan * np.empty((X.shape[1], scores.shape[1]))
        for i in range(X.shape[1]):
            for j in range(scores.shape[1]):
                # scores are assumed to be always valid
                is_finite = finite_X[:, i]  #~(X.mask[:, i])
                loadings[i, j] = pearsonr(
                    X[is_finite, i], scores[is_finite, j])[0]
    return loadings


def analyze_resampled(estr, X, Y, Xorig=None, Yorig=None,
                      x_align_ref=None, y_align_ref=None,
                      addons=tuple(), resample_addons=None,
                      n_bs=0, perm=None, loo=False,
                      random_state=None,
                      saved_perm_features='all',
                      show_progress=True,
                      n_jobs=1,
                      fit_params=None,
                      **kwargs):
    """Analyze a given dataset and resampled versions of it with a given
    estimator.

    In addition to the dataset itself resampled versions of it are analyzed in
    the same way. Provided resampling strategies include:

    * bootstrap
    * leave-one-out
    * permutation

    Parameters
    ----------
    estr : **sklearn**-style estimator
        for performing CCA or PLS. Must have method ``fit`` and (after fitting)
        attributes ``assocs_``, ``x_rotations_``, ``y_rotations_``,
        ``x_scores_``, ``y_scores_``
    X : np.ndarray (n_samples, n_features)
        dataset `X`
    Y : np.ndarray (n_samples, n_features)
        dataset `Y`
    Xorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `X`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``X`` and ``Xorig`` correspond to the same samples
        (subjects).
    Yorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `Y`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``Y`` and ``Yorig`` correspond to the same samples
        (subjects).
    x_align_ref : (n_features,)
        after fitting, the sign of `X` weights is chosen such that the
        cosine-distance between fitted `X` weights and ``x_align_ref`` is
        positive
    y_align_ref : (n_features,)
        after fitting, the sign of `Y` weights is chosen such that the
        cosine-distance between fitted `Y` weights and ``y_align_ref`` is
        positive
    addons : list-like of add-on functions
        After fitting the estimator and saving association strengths, weights
        and loadings in ``results`` additional analyses can be performed with
        these functions. They are called in the given order, and must have the
        signature

        .. code-block:: python

            addana_fun(estr, X, Y, Xorig, Yorig, x_align_ref, y_align_ref,
                results, **kwargs)

        and are expected to save their respective outcome features ``results``.
        Various such functions are provided in module
        :mod:`sample_analysis_addons`
    resample_addons : None or list-like of add-on functions
        if ``None`` then ``addons`` is used
    n_bs : int
        number of bootstrap iterations to perform on the data
    perm : None, int, or iterable
        if ``None`` no permutations are performed, if ``int`` gives the number
        of permutations to perform on the data, if iterable each element is an
        array giving the permuted indices to use
    loo : bool
        if ``True`` leave-one-out analysis is performed on the data
    random_state : ``None``, int or random number generator instance
        used to generate random numbers
    saved_perm_features : 'all' or list-like of 'str'
        if 'all' all outcome features resulting from the permutations are
        returned, otherwise only data variables indicated by this list are
    show_progress : bool
        whether to show progress bar
    n_jobs : int or None
        number of parallel jobs (see :class:`joblib.Parallel`)
    fit_params : dict
        keyword-arguments for estr.fit
    kwargs : dict
        forwarded to additional analysis functions

    Returns
    -------
    results : xr.Dataset
        containing data variables for outcome features generated by analyses.
        Data variables obtained from bootstrapping are suffixed '_bs', from
        permutations '_perm' and from leave-one-out '_loo'
    """

    _tqdm = _prep_progressbar(show_progress)

    if resample_addons is None:
        resample_addons = addons

    parallel = Parallel(n_jobs=n_jobs)

    results = analyze_dataset(estr, X, Y, Xorig, Yorig, x_align_ref,
                              y_align_ref, addons, fit_params, **kwargs)

    rng = check_random_state(random_state)

    if n_bs > 0:

        if x_align_ref is None:
            x_align_ref_ = results.x_weights.values
        else:
            x_align_ref_ = x_align_ref

        if y_align_ref is None:
            y_align_ref_ = results.y_weights.values
        else:
            y_align_ref_ = y_align_ref

        results_bs = []
        min_unique_inds = min(X.shape[1], Y.shape[1]) + 1
        for bsi in _tqdm(range(n_bs), total=n_bs, leave=False,
                         desc='bootstrap'):
            bs_inds = np.array([])
            # make sure that there are more UNIQUE bs-samples than features so
            # that cov-matrix has full rank
            while len(np.unique(bs_inds)) < min_unique_inds:
                bs_inds = rng.choice(np.arange(len(X)), size=len(X),
                                     replace=True)
            results_bs.append(
                analyze_dataset(
                    estr,
                    X[bs_inds], Y[bs_inds],
                    _maybe_slice(Xorig, bs_inds), _maybe_slice(Yorig, bs_inds),
                    x_align_ref=x_align_ref_, y_align_ref=y_align_ref_,
                    addons=resample_addons, fit_params=fit_params, **kwargs
                )
            )

        results_bs = xr.concat(results_bs, 'bs')

        for v in results_bs.data_vars:
            results[v + '_bs'] = results_bs[v]

    if loo:

        if x_align_ref is None:
            x_align_ref_ = results.x_weights.values
        else:
            x_align_ref_ = x_align_ref

        if y_align_ref is None:
            y_align_ref_ = results.y_weights.values
        else:
            y_align_ref_ = y_align_ref

        results_loo = []
        for looi in _tqdm(range(len(X)), total=len(X), leave=False,
                          desc='leave-one-out'):
            loo_inds = np.ones(len(X), dtype=bool)
            loo_inds[looi] = False
            results_loo.append(
                analyze_dataset(
                    estr, X[loo_inds], Y[loo_inds],
                    _maybe_slice(Xorig, loo_inds),
                    _maybe_slice(Yorig, loo_inds),
                    x_align_ref=x_align_ref_, y_align_ref=y_align_ref_,
                    addons=resample_addons, fit_params=fit_params, **kwargs
                )
            )

        results_loo = xr.concat(results_loo, 'loo')

        for v in results_loo.data_vars:
            results[v + '_loo'] = results_loo[v]

    if (perm is not None) and \
            ((not isinstance(perm, numbers.Integral)) or perm > 0):

        if isinstance(perm, numbers.Integral):
            n_permutations = perm
            perm_iter = _naive_permutations(Y, rng, n_permutations)
        else:
            n_permutations = len(perm)
            perm_iter = perm

        def _ana_ds_perm(perm_inds):
            return analyze_dataset(
                estr, X, Y[perm_inds], Xorig, _maybe_slice(Yorig, perm_inds),
                # for permuted data do not align
                # x_align_ref=None, y_align_ref=None,
                x_align_ref=x_align_ref,
                y_align_ref=y_align_ref,
                addons=resample_addons, fit_params=fit_params, **kwargs
            )
        results_perm = parallel(
            delayed(_ana_ds_perm)(perm_inds)
            for perm_inds in perm_iter
        )

        results_perm = xr.concat(results_perm, 'perm')

        if saved_perm_features == 'all':
            saved_perm_features = results_perm.data_vars
        # else: assume saved_perm_features is a list of names
        for v in saved_perm_features:
            results[v + '_perm'] = results_perm[v]

    return results


def _naive_permutations(Y, rng, n):
    for _ in range(n):
        yield rng.permutation(len(Y))


def _maybe_slice(X, slice):
    if X is None:
        return X
    else:
        return X[slice]


def analyze_subsampled(estr, X, Y, Xorig=None, Yorig=None, x_align_ref=None,
                       y_align_ref=None, addons=tuple(), ns=tuple(), n_rep=10,
                       n_perm=100, n_test=0, postprocessors=tuple(), n_jobs=1,
                       show_progress=True, random_state=None, fit_params=None,
                       overlapping_subjects=True, **kwargs):
    """Analyze subsampled versions of a dataset with a given estimator.

    Parameters
    ----------
    estr : **sklearn**-style estimator
        for performing CCA or PLS. Must have method ``fit`` and (after fitting)
        attributes ``assocs_``, ``x_rotations_``, ``y_rotations_``,
        ``x_scores_``, ``y_scores_``
    X : np.ndarray (n_samples, n_features)
        dataset `X`
    Y : np.ndarray (n_samples, n_features)
        dataset `Y`
    Xorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `X`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``X`` and ``Xorig`` correspond to the same samples
        (subjects).
    Yorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `Y`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``Y`` and ``Yorig`` correspond to the same samples
        (subjects).
    x_align_ref : (n_features,)
        after fitting, the sign of `X` weights is chosen such that the
        cosine-distance between fitted `X` weights and ``x_align_ref`` is
        positive
    y_align_ref : (n_features,)
        after fitting, the sign of `Y` weights is chosen such that the
        cosine-distance between fitted `Y` weights and ``y_align_ref`` is
        positive
    addons : list-like of add-on functions
        After fitting the estimator and saving association strengths, weights
        and loadings in ``results`` additional analyses can be performed with
        these functions. They are called in the given order, and must have the
        signature

        .. code-block:: python

            addana_fun(estr, X, Y, Xorig, Yorig, x_align_ref, y_align_ref,
                results, **kwargs)

        and are expected to save their respective outcome features ``results``.
        Various such functions are provided in module
        :mod:`sample_analysis_addons`
    ns : list-like of int
        subsamples of these sizes are used
    n_rep : int
        number of times a subsample of a given size is drawn
    n_perm : int
        each subsample is permuted ``n_perm`` times to generate a
        null-distribution of outcome quantities
    n_test : int or 'auto'
        number of subjects to use as test set. ``max(ns) + n_test`` must be <=
        ``n_samples``. If ``n_test == 'auto'`` then
        ``n_test = n_samples - max(ns)`` will be used.
    postprocessors : list-like of functions
        functions are called after the final dataset has been concatenated and
        take that xr.Dataset as only argument
    n_jobs : int or None
        number of parallel jobs (see :class:`joblib.Parallel`)
    show_progress : bool
        whether to show progress bar
    random_state : ``None``, int or random number generator instance
        used to generate random numbers
    fit_params : dict
        keyword-arguments for estr.fit
    overlapping_subjects : bool
        if ``True`` allow overlapping subjects in different repetitions. If
        ``False``, this implies that ``max(ns)`` must be smaller than
        ``(len(X) - n_test) / n_rep``.
    kwargs : dict
        forwarded to additional analysis functions

    Returns
    -------
    results : xr.Dataset
        containing data variables for outcome features generated by analyses
    """

    _tqdm = _prep_progressbar(show_progress)

    rng = check_random_state(random_state)
    n_subjects = len(X)

    if (len(ns) == 0) or (n_rep <= 0):
        raise ValueError('Nothing to do, need "ns" and nrep > 0')
    if not (np.asarray(ns) < n_subjects).all():
        raise ValueError('All subsample sizes must be smaller than total '
                         'number of available samples ({})'.format(n_subjects))

    if n_test == 'auto':
        n_test = n_subjects - max(ns)
    elif isinstance(n_test, numbers.Integral):
        pass  # use n_test as is
    else:
        raise ValueError(f'Invalid n_test: {n_test}')

    if max(ns) + n_test > n_subjects:
        raise ValueError('max(ns) + n_test must be <= n_samples')

    permuted_subjects = rng.permutation(n_subjects)
    train_subjects = permuted_subjects[n_test:]
    test_subjects = permuted_subjects[:n_test]
    Xtest, Ytest = X[test_subjects], Y[test_subjects]
    Xorigtest = _maybe_slice(Xorig, test_subjects)
    Yorigtest = _maybe_slice(Yorig, test_subjects)

    if not overlapping_subjects:
        if max(ns) > len(train_subjects) / n_rep:
            raise ValueError("max(ns) must be smaller than "
                             "len(train_subjects) / n_rep")

    results_n = []
    for n in _tqdm(ns, total=len(ns), desc='n', leave=False):

        results_rep = []
        for repi in _tqdm(range(n_rep), total=n_rep, leave=False, desc='rep'):

            if overlapping_subjects:
                inds = rng.permutation(train_subjects)[:n]
            else:
                inds = train_subjects[repi*n:(repi+1)*n]

            X_, Y_ = X[inds], Y[inds]
            Xorig_ = _maybe_slice(Xorig, inds)
            Yorig_ = _maybe_slice(Yorig, inds)

            results_rep.append(
                analyze_resampled(
                    estr, X_, Y_, Xorig_, Yorig_,
                    x_align_ref=x_align_ref, y_align_ref=y_align_ref,
                    perm=n_perm, random_state=rng,
                    addons=addons, n_jobs=n_jobs,
                    Xtest=Xtest, Ytest=Ytest,
                    Xorigtest=Xorigtest, Yorigtest=Yorigtest,
                    fit_params=fit_params, **kwargs
                )
            )

        results_n.append(
            xr.concat(results_rep, 'rep')
        )

    results_n = xr.concat(results_n, pd.Index(ns, name='n'))

    for postproc in postprocessors:
        postproc(results_n)

    return results_n


def analyze_splithalf(estr, X, Y, Xorig=None, Yorig=None, x_align_ref=None,
                      y_align_ref=None, addons=tuple(), ns=tuple(), n_rep=10,
                      n_perm=100, n_test=0, postprocessors=tuple(), n_jobs=1,
                      show_progress=True, random_state=None, fit_params=None,
                      **kwargs):
    """Analyze split-half versions of a dataset with a given estimator.

    Parameters
    ----------
    estr : **sklearn**-style estimator
        for performing CCA or PLS. Must have method ``fit`` and (after fitting)
        attributes ``assocs_``, ``x_rotations_``, ``y_rotations_``,
        ``x_scores_``, ``y_scores_``
    X : np.ndarray (n_samples, n_features)
        dataset `X`
    Y : np.ndarray (n_samples, n_features)
        dataset `Y`
    Xorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `X`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``X`` and ``Xorig`` correspond to the same samples
        (subjects).
    Yorig : ``None`` or np.ndarray (n_samples, n_orig_features)
        can be ``None``. Allows to provide an alternative set of `Y`
        features for calculating loadings. I.e. an implicit assumption is that
        the rows in ``Y`` and ``Yorig`` correspond to the same samples
        (subjects).
    x_align_ref : (n_features,)
        after fitting, the sign of `X` weights is chosen such that the
        cosine-distance between fitted `X` weights and ``x_align_ref`` is
        positive
    y_align_ref : (n_features,)
        after fitting, the sign of `Y` weights is chosen such that the
        cosine-distance between fitted `Y` weights and ``y_align_ref`` is
        positive
    addons : list-like of add-on functions
        After fitting the estimator and saving association strengths, weights
        and loadings in ``results`` additional analyses can be performed with
        these functions. They are called in the given order, and must have the
        signature

        .. code-block:: python

            addana_fun(estr, X, Y, Xorig, Yorig, x_align_ref, y_align_ref,
                results, **kwargs)

        and are expected to save their respective outcome features ``results``.
        Various such functions are provided in module
        :mod:`sample_analysis_addons`
    ns : list-like of int
        subsamples of these sizes are used
    n_rep : int
        number of times a split-half pair is formed and analyzed
    n_perm : int
        each subsample is permuted ``n_perm`` times to generate a
        null-distribution of outcome quantities
    n_test : int or 'auto'
        number of subjects to use as test set. ``max(ns) + n_test`` must be <=
        ``n_samples``. If ``n_test == 'auto'`` then
        ``n_test = n_samples - max(ns)`` will be used.
    postprocessors : list-like of functions
        functions are called after the final dataset has been concatenated and
        take that xr.Dataset as only argument
    n_jobs : int or None
        number of parallel jobs (see :class:`joblib.Parallel`)
    show_progress : bool
        whether to show progress bar
    random_state : ``None``, int or random number generator instance
        used to generate random numbers
    fit_params : dict
        keyword-arguments for estr.fit
    kwargs : dict
        forwarded to additional analysis functions

    Returns
    -------
    results : xr.Dataset
        containing data variables for outcome features generated by analyses
    """

    # IMPORTANT: pass rng (NOT random_state) to analyzed_subsampled to make
    # sure a different seed is used for each rep
    rng = check_random_state(random_state)

    analyzed_splithalf_pairs = [
        analyze_subsampled(estr, X, Y, Xorig=Xorig, Yorig=Yorig,
                           x_align_ref=x_align_ref, y_align_ref=y_align_ref,
                           addons=addons, ns=ns, n_rep=2, n_perm=n_perm,
                           n_test=n_test, postprocessors=postprocessors,
                           n_jobs=n_jobs, show_progress=show_progress,
                           random_state=rng, fit_params=fit_params,
                           overlapping_subjects=False, **kwargs
                           ).rename(rep='splithalf')
        for _ in trange(n_rep, desc='splithalf_pair', leave=False)
    ]
    return xr.concat(analyzed_splithalf_pairs, 'rep')


def analyze_model_light(estr, Sigma, px, ns, x_align_ref=None,
                        y_align_ref=None, addons=tuple(), n_rep=10, n_perm=100,
                        random_state=None, fit_params=None, **kwargs):
    """Synthetic datasets drawn from a model are analyzed with a given
    estimator.

    The model is specified by the covariance matrix ``Sigma``. Synthetic
    datasets are drawn from the corresponding normal distribution and analyzed.

    Parameters
    ----------
    estr : **sklearn**-style estimator
        for performing CCA or PLS. Must have method ``fit`` and (after fitting)
        attributes ``assocs_``, ``x_rotations_``, ``y_rotations_``,
        ``x_scores_``, ``y_scores_``
    Sigma : (total number of features in `X` and `Y`, total number of features
        in `X` and `Y`) model covariance matrix
    px : int
        number of `X` features (number of `Y` features is inferred from size of
        ``Sigma``)
    ns : list-like of int
        datasets of these sizes are generated from the model and analyzed
    x_align_ref : (n_features,)
        after fitting, the sign of `X` weights is chosen such that the
        cosine-distance between fitted `X` weights and ``x_align_ref`` is
        positive
    y_align_ref : (n_features,)
        after fitting, the sign of `Y` weights is chosen such that the
        cosine-distance between fitted `Y` weights and ``y_align_ref`` is
        positive
    addons : list-like of add-on functions
        After fitting the estimator and saving association strengths, weights
        and loadings in ``results`` additional analyses can be performed with
        these functions. They are called in the given order, and must have the
        signature

        .. code-block:: python

            addana_fun(estr, X, Y, Xorig, Yorig, x_align_ref, y_align_ref,
                results, **kwargs)

        and are expected to save their respective outcome features ``results``.
        Various such functions are provided in module
        :mod:`sample_analysis_addons`
    n_rep : int
        number of times a dataset of a given size is generated
    n_perm : int
        each generated dataset is permuted ``n_perm`` times to generate a
        null-distribution of outcome quantities
    random_state : ``None``, int or random number generator instance
        used to generate random numbers
    fit_params : dict
        keyword-arguments for estr.fit
    kwargs : dict
        forwarded to additional analysis functions

    Returns
    -------
    results : xr.Dataset
        containing data variables for outcome features generated by analyses
    """

    rng = check_random_state(random_state)

    if (len(ns) == 0) or (n_rep <= 0):
        raise ValueError('Nothing to do, need "ns" and nrep > 0')

    results_n = []
    for n in ns:

        results_rep = []
        for repi in range(n_rep):

            X, Y = generate_data(Sigma, px, n, rng)

            results_rep.append(
                analyze_resampled(
                    estr, X, Y,
                    x_align_ref=x_align_ref, y_align_ref=y_align_ref,
                    perm=n_perm, random_state=rng,
                    addons=addons, fit_params=fit_params, **kwargs
                )
            )

        results_n.append(
            xr.concat(results_rep, 'rep')
        )

    results_n = xr.concat(results_n, pd.Index(ns, name='n'))
    return results_n


def _check_powerlaw_decay(n_Sigmas, rng, powerlaw_decay=(-1, -1)):
    if len(powerlaw_decay) == 2:
        for _ in range(n_Sigmas):
            yield powerlaw_decay[0], powerlaw_decay[1]
    elif (len(powerlaw_decay) == 3) and (powerlaw_decay[0] == 'random_sum'):
        if powerlaw_decay[1] > powerlaw_decay[2]:
            raise ValueError(
                'When ``powerlaw_decay[0] == "random_sum"`` then '
                '``powerlaw_decay[1]`` must be <= ``powerlaw_decay[2]``. '
                'Got: {}'.format(powerlaw_decay))
        for _ in range(n_Sigmas):
            _random_sum = rng.uniform(*powerlaw_decay[1:3])
            ax = rng.uniform(0, 1) * _random_sum
            ay = _random_sum - ax
            yield ax, ay
    else:
        raise ValueError('Invalid powerlaw_decay specification')


def analyze_model_parameters(model, estr=None, n_rep=100, n_bs=0, n_perm=0,
                             n_per_ftrs=(2, 10, 50), pxs=(4, 8, 16, 32, 64),
                             pys='px', rs=(.1, .3, .5, .7,),
                             n_Sigmas=1,
                             powerlaw_decay=(-1, -1), coordinate_system='pc',
                             expl_var_ratio_thr=1./2,
                             max_n_sigma_trials=100000, addons=tuple(),
                             resample_addons=None,
                             n_test=0, mk_test_statistics=None,
                             postprocessors=tuple(), comparison_gms=tuple(),
                             random_state=0, show_progress=True,
                             check_convergence=False, conv_thr=0.99,
                             fit_params=None, **kwargs):
    """Parameter-dependent models are set up and resulting synthetic datasets
    are analyzed.

    For each model, differing by the number of features, ground-truth
    correlations, within-set principal component spectra and direction of
    weight vectors relative to the principal component axes, synthetic datasets
    are generated and analyzed.

    Parameters
    ----------
    model : 'cca' or 'pls'
        whether synthetic data is generated and analyzed for CCA or PLS
    estr : ``None`` or **sklearn**-style estimator
        if ``None`` either :class:`.estimators.SVDCCA` or
        :class:`.estimators.SVDPLS` is used depending on the value of
        ``model``. Otherwise, the given estimator should correspond to
        ``model`` and must have a method ``fit`` and (after fitting) attributes
        ``assocs_``, ``x_rotations_``, ``y_rotations_``, ``x_scores_``,
        ``y_scores_``
    n_rep : int
        for each investigated model (i.e. for each joint covariance matrix)
        specified by the number of features (argument ``pxs``), ground-truth
        correlations (argument ``rs``) and the principal component spectra
        (argument ``n_Sigmas``) ``n_rep`` datasets are drawn from this
        particular model and analyzed
    n_bs : int
        number of bootstrap iterations to perform on each synthetic dataset
    n_perm : int
        number of permutations to perform on each synthetic dataset
    n_per_ftrs : 'auto' or list-like of int
        multiplied by ``px+py`` specifies the size of samples generated from
        the model. If 'auto' values
        are chosen heuristically.
    pxs : list-like of int
        number of `X`-features to use
    pys : 'px', function or int
        if 'px' uses ``px`` `Y` features, if function uses ``function(px)``
        `Y` features, if int uses ``int(pys)`` `Y` features
    rs : list-like of float between 0 and 1
        assumed ground-truth correlations
    n_Sigmas : int
        number of covariance matrices generated for each `px` and `r`. Given
        `px` and `r` covariance matrices differ by their within-set principal
        component spectra (specified by parameter ``powerlaw_decay`` and the
        directions of the between-set mode (i.e. CCA  / PLS weight) vectors
        relative to the principal component axes
    powerlaw_decay : tuple of floats <= 0
        separately for `X` and `Y` the within-set principal component spectrum
        is assumed to follow a power-law. ``powerlaw_decay`` can either be a
        tuple of 2 floats <= 0, in which case the 2 numbers represent the
        decay constants for `X` and `Y`, respectively. Alternatively,
        ``powerlaw_decay`` can be a tuple comprising the string `random_sum`
        and 2 floats <= 0, in which case the value for the **sum** of the
        decay constants for `X` and `Y` is drawn from a uniform distribution
        with boundaries given by the 2 floats; the decay constant for `X` is
        then a random fraction (uniform between 0 and 1) of the sum, and the
        decay constant for `Y` is such that the 2 decay constants sum up to the
        value for the sum
    coordinate_system : bool
        if ``True`` a random rotation is applied to each generated dataset, the
        same rotation is applied to datasets drawn from the same model (i.e.
        with the same joint covariance matrix)
    expl_var_ratio_thr : float
        threshold for required within-modality variance along latent mode
        vectors
    max_n_sigma_trials : int
        number of times an attempt is made to find suitable latent mode
        vectors. See `_mk_Sigmaxy` for details.
    addons : list-like of functions
        After fitting the estimator and saving association strengths, weights
        and loadings in ``results`` additional analyses can be performed with
        these functions. They are called in the given order, and must have the
        signature

        .. code-block:: python

            addana_fun(estr, X, Y, Xorig, Yorig, x_align_ref, y_align_ref,
                results, **kwargs)

        and are expected to save their respective outcome features ``results``.
        Various such functions are provided in module
        :mod:`sample_analysis_addons`
    resample_addons : None or list-like of add-on functions
        if ``None`` then ``addons`` is used
    n_test : 'auto' or int >= 0
        to analyze some consistency properties across repeated draws from the
        same model, a test set of size ``n_test`` is generated for each joint
        covariance matrix and provided to down-stream analyses via
        keyword-arguments ``Xtest`` and ``Ytest``. If set to ``'auto'`` a test
        set of size max(n_per_ftrs) * (px + py) is used.
    mk_test_statistics : ``None`` or function
        if not ``None`` the function must have the signature

        .. code-block:: python

            fun(Xtest, Ytest, x_weights_true, y_weights_true)

        where ``Xtest`` and ``Ytest`` are, respectively, ``np.ndarray`` of
        dimension ``(n_test, n_x_features)`` and ``(n_test, n_y_features)``,
        and ``x_weights_true`` and ``y_weights_true`` are ``np.ndarray`` of
        dimension ``(n_x_features, n_components)`` and
        ``(n_y_features, n_components)`` containing the true weight vectors
    postprocessors : list-like of functions
        functions are called after the final dataset has been concatenated and
        take that xr.Dataset as only argument
    comparison_gms : list-like of tuples (label, functions)
        labels identify the alternative generative models, functions return an
        object that encodes the alternative generative models, based on a given
        one. Functions take a GEMMR instance as only positional argument and
        ``m`` (number of between-set modes) and ``random_state`` (a random
        number generator instance) as keyword arguments, and return an
        instance of an object that has essentially the same attributes as gm
        (cf source of :func:`analyze_model` to see which are used). The
        dimensionalities of the the `X` and `Y` latent spaces must be
        identical to those of the GEMMR instance. Every generated dataset will
        then additionally be analyzed here with the estimator ``estr`` and
        with respect to the ground truth latent axes ``x_weights_`` and
        ``y_weights_`` encoded in these returned objects.
    random_state : ``None``, int or random number generator instance
        used to generate random numbers    additional_analyses
    show_progress : bool
        if ``True`` progress bars are shown, if ``False`` not
    fit_params : dict
        keyword-arguments for estr.fit
    kwargs : dict
        forwarded to additional analysis functions

    Returns
    -------
    results : xr.Dataset
        containing data variables for outcome features generated by analyses
    """

    if 'axPlusay_range' in kwargs:
        raise ValueError("kwarg axPlusay_range is deprecated, use "
                         "powerlaw_decay")

    _tqdm = _prep_progressbar(show_progress)

    estr = _check_model_and_estr(model, estr)

    rng = check_random_state(random_state)

    px_results = []
    for pxi, px in _tqdm(enumerate(pxs), total=len(pxs), desc='px'):

        # px = px
        py = _get_py(pys, px)

        r_results = []
        for ri, r_between in _tqdm(enumerate(rs), total=len(rs), leave=False,
                                   desc='r'):

            sigma_results = []
            for sigmai, (ax, ay) in _tqdm(enumerate(
                    _check_powerlaw_decay(n_Sigmas, rng, powerlaw_decay)
            ), total=n_Sigmas, leave=False, desc='Sigma'):
                try:
                    gm = GEMMR(model, random_state=rng,
                               wx=px, wy=py,
                               ax=ax, ay=ay, r_between=r_between,
                               expl_var_ratio_thr=expl_var_ratio_thr,
                               max_n_sigma_trials=max_n_sigma_trials,
                               coordinate_system=coordinate_system)
                except ValueError as e:
                    warnings.warn("Couldn't find Sigma: {}".format(e),
                                  category=JointCovarianceWarning)
                    continue

                my_comparison_gms = [(lbl, estr,
                                      mk_gm(gm, random_state=rng))
                                     for lbl, estr, mk_gm in comparison_gms]

                n_per_ftrs = _select_n_per_ftrs(n_per_ftrs, model, ax, ay,
                                                r_between)

                _sigma_results = analyze_model(gm, estr, n_per_ftrs=n_per_ftrs,
                                               n_rep=n_rep, n_perm=n_perm,
                                               n_bs=n_bs, n_test=n_test,
                                               mk_test_statistics=mk_test_statistics,
                                               addons=addons,
                                               resample_addons=resample_addons,
                                               postprocessors=postprocessors,
                                               comparison_gms=my_comparison_gms,
                                               random_state=rng,
                                               show_progress=show_progress,
                                               fit_params=fit_params,
                                               check_convergence=check_convergence,
                                               conv_thr=conv_thr,
                                               **kwargs)

                sigma_results.append(
                    _sigma_results
                )

            r_results.append(xr.concat(
                sigma_results,
                pd.Index(np.arange(len(sigma_results)), name='Sigma_id')
            ))

        _r_results = xr.concat(r_results, pd.Index(rs, name='r'))
        _r_results['py'] = py
        px_results.append(
            _r_results
        )

    result = xr.concat(px_results, pd.Index(pxs, name='px'))

    result = result.sortby(['n_per_ftr', 'px', 'r'])

    # add some metadata
    result.attrs['model'] = model
    result.attrs['estr'] = repr(estr)
    result.attrs['powerlaw_decay'] = powerlaw_decay
    result.attrs['created'] = str(datetime.now())
    from .. import __version__ as gemmr_version
    result.attrs['gemmr_version'] = gemmr_version

    return result


def analyze_model(gm, estrs, n_per_ftrs=(3, 4, 8, 16, 32, 64, 128, 256, 512),
                  n_rep=100, n_perm=0, n_bs=0, n_test=0,
                  mk_test_statistics=None,
                  addons=tuple(), resample_addons=None, postprocessors=tuple(),
                  comparison_gms=tuple(), random_state=0, show_progress=True,
                  add_metadata=False, fit_params=None,
                  check_convergence=False, conv_thr=0.99,
                  **kwargs):

    _tqdm = _prep_progressbar(show_progress)
    rng = check_random_state(random_state)

    if not isinstance(estrs, (list, tuple)):
        estrs = [estrs]

    true_loadings = _calc_true_loadings(gm.Sigma_, gm.px,
                                        gm.x_weights_, gm.y_weights_)
    true_loadings_cmp_gms = [
        _calc_true_loadings(cgm.Sigma_, cgm.px, cgm.x_weights_, cgm.y_weights_)
        for _, _, cgm in comparison_gms
    ]

    if (n_test == 'auto') or (n_test > 0):
        n_max = np.max(n_per_ftrs) * (gm.px + gm.py)
        if n_test == 'auto':
            n_test = n_max
        elif n_max > n_test:
            warnings.warn(
                "It's recommended to have n_test > max n, but got "
                "n_test ({}) < max n ({}).".format(n_test, n_max),
                category=UserWarning)

    Xtest, Ytest = gm.generate_data(n_test, random_state=rng)

    if mk_test_statistics is None:
        test_stats = dict()
        test_stats_cmp_gms = [dict() for _, _, cgm in comparison_gms]
    else:
        test_stats = mk_test_statistics(
            Xtest, Ytest, gm.x_weights_, gm.y_weights_)
        test_stats_cmp_gms = [
            mk_test_statistics(Xtest, Ytest, cgm.x_weights_, cgm.y_weights_)
            for _, _, cgm in comparison_gms
        ]

    n_results = []
    n_results_cmp_gms = [[] for _ in comparison_gms]
    for ni, n_per_ftr in _tqdm(
            enumerate(n_per_ftrs),
            total=len(n_per_ftrs), leave=False, desc='n_per_ftr'
    ):

        # px + py = total number of variables in X and Y
        n = int((gm.px + gm.py) * n_per_ftr)

        rep_results = []
        rep_results_cmp_gms = [[] for _ in comparison_gms]
        for repi in _tqdm(range(n_rep), total=n_rep, leave=False,
                          desc='repetition'):
            X, Y = gm.generate_data(n, random_state=rng)

            estr_results = []
            for estr in estrs:
                ds = analyze_resampled(
                    estr, X, Y, Xorig=None, Yorig=None,
                    n_bs=n_bs, perm=n_perm, random_state=rng,
                    addons=addons, resample_addons=resample_addons,
                    x_align_ref=gm.x_weights_, y_align_ref=gm.y_weights_,
                    true_loadings=true_loadings,
                    Xtest=Xtest, Ytest=Ytest, Xorigtest=None, Yorigtest=None,
                    test_statistics=test_stats,
                    show_progress=show_progress, fit_params=fit_params, **kwargs
                )
                estr_results.append(ds)

            if len(estrs) == 1:
                rep_results.append(estr_results[0])
            else:
                rep_results.append(
                    xr.concat(estr_results, pd.Index(
                        [e.__class__.__name__ for e in estrs], name='estr'))
                )

            for ci, (cmp_gm_lbl, cmp_estr, cmp_gm) in enumerate(comparison_gms):

                estr_results = []
                for estr in estrs:
                    ds_cmp_gm = analyze_resampled(
                        cmp_estr, X, Y, Xorig=None, Yorig=None,
                        n_bs=n_bs, perm=n_perm, random_state=rng,
                        addons=addons, resample_addons=resample_addons,
                        x_align_ref=cmp_gm.x_weights_, y_align_ref=cmp_gm.y_weights_,
                        true_loadings=true_loadings_cmp_gms[ci],
                        Xtest=Xtest, Ytest=Ytest,
                        test_statistics=test_stats_cmp_gms[ci],
                        show_progress=show_progress, fit_params=fit_params,
                        **kwargs
                    )
                    estr_results.append(ds_cmp_gm)

                if len(estrs) == 1:
                    rep_results_cmp_gms[ci].append(estr_results[0])
                else:
                    rep_results_cmp_gms[ci].append(
                        xr.concat(estr_results, pd.Index(
                            [e.__class__.__name__ for e in estrs],
                            name='estr'))
                    )

        n_results.append(
            xr.concat(rep_results, 'rep')
        )
        for ci in range(len(comparison_gms)):
            n_results_cmp_gms[ci].append(
                xr.concat(rep_results_cmp_gms[ci], 'rep')
            )

        if check_convergence and \
            (n_results[-1].x_weights_true_cossim.mean('rep') >
                conv_thr).all() and \
            (n_results[-1].y_weights_true_cossim.mean('rep') >
                conv_thr).all():
            break

    # --- add properties of gm to results ---
    results = _combine_outcomes_and_postproc(
        n_results, gm, n_per_ftrs, test_stats, true_loadings, postprocessors
    )
    results_cmp_gms = [
        _combine_outcomes_and_postproc(
            n_results_cmp_gms[ci], cmp_gm, n_per_ftrs, test_stats_cmp_gms[ci],
            true_loadings_cmp_gms[ci], postprocessors
        )
        for ci, (cmp_gm_lbl, _, cmp_gm) in enumerate(comparison_gms)
    ]

    # --- combine all results ---
    for ci, (cmp_gm_lbl, _, cmp_gm) in enumerate(comparison_gms):
        for v in results_cmp_gms[ci]:
            results[f'{cmp_gm_lbl}_{v}'] = results_cmp_gms[ci][v]

    if add_metadata:
        # add some metadata
        result.attrs['estr'] = repr(estr)
        result.attrs['created'] = str(datetime.now())
        from .. import __version__ as gemmr_version
        result.attrs['gemmr_version'] = gemmr_version

    return results


def _combine_outcomes_and_postproc(n_results, gm, n_per_ftrs, test_stats,
                                   true_loadings, postprocessors):

    coords_x_ftr = dict(x_feature=np.arange(len(gm.x_weights_)))
    coords_y_ftr = dict(y_feature=np.arange(len(gm.y_weights_)))

    results = xr.concat(
        n_results, pd.Index(n_per_ftrs[:len(n_results)], name='n_per_ftr'))

    results['between_assocs_true'] = \
        xr.DataArray(gm.true_assocs_, dims=('mode',))
    results['between_corrs_true'] = \
        xr.DataArray(gm.true_corrs_, dims=('mode',))

    results['x_weights_true'] = \
        xr.DataArray(gm.x_weights_, dims=('x_feature', 'mode'),
                     coords=coords_x_ftr)
    results['y_weights_true'] = \
        xr.DataArray(gm.y_weights_, dims=('y_feature', 'mode'),
                     coords=coords_y_ftr)

    results['ax'] = gm.ax
    results['ay'] = gm.ay

    results['x_loadings_true'] = \
        xr.DataArray(true_loadings['x_loadings_true'],
                     dims=('x_feature', 'mode'), coords=coords_x_ftr)
    results['x_crossloadings_true'] = \
        xr.DataArray(true_loadings['x_crossloadings_true'],
                     dims=('x_feature', 'mode'), coords=coords_x_ftr)
    results['y_loadings_true'] = \
        xr.DataArray(true_loadings['y_loadings_true'],
                     dims=('y_feature', 'mode'), coords=coords_y_ftr)
    results['y_crossloadings_true'] = \
        xr.DataArray(true_loadings['y_crossloadings_true'],
                     dims=('y_feature', 'mode'), coords=coords_y_ftr)

    for k, v in test_stats.items():
        results[k] = v

    for postproc in postprocessors:
        postproc(results)

    return results


def _prep_progressbar(show_progress):
    if show_progress:
        _tqdm = tqdm
    else:
        def _tqdm(x, *args, **kwargs):
            return x
    return _tqdm


def _check_model_and_estr(model, estr):

    if model not in ['pls', 'cca']:
        raise ValueError(
            'Invalid model: {}. Must be "pls" or "cca"'.format(model))

    if (estr is None) or (estr == 'auto'):
        if (model == 'pls'):
            estr = 'PLS'  # see below
        elif (model == 'cca'):
            estr = 'CCA'  # see below

    if isinstance(estr, str):
        if estr.lower() == 'cca':
            estr = SVDCCA(n_components=1, scale=False, std_ddof=1,
                          normalize_weights=True, cov_out_of_bounds='raise',
                          calc_loadings=True)
        elif estr.lower() == 'pls':
            estr = SVDPLS(n_components=1, scale=False, std_ddof=1,
                          calc_loadings=True)
        elif estr.lower() == 'sparsepls':
            estr = SparsePLS(n_components=1, scale=False, std_ddof=1)
        else:
            raise ValueError("Unknown estimator type: '{}'. Must be 'auto', "
                             "'CCA', 'PLS' or 'SparsePLS'".format(estr))
    else:
        # heuristic check if estr class is compatible with model

        if not (hasattr(estr, 'n_components') and hasattr(estr, 'fit')):
            raise ValueError("Estimator ({}) doesn't have required attributes "
                             "'n_components' or 'fit'".format(estr))

        estr_name = estr.__class__.__name__.lower()
        if 'pls' in estr_name:
            if model != 'pls':
                warnings.warn("requested model is 'pls', but estimator doesn't"
                              " seem to be a PLS estimator",
                              category=UserWarning)
        elif 'cca' in estr_name:
            if model != 'cca':
                warnings.warn("requested model is 'cca', but estimator doesn't"
                              " seem to be a CCA estimator",
                              category=UserWarning)
        else:
            warnings.warn("requested model doesn't seem to be a PLS or CCA"
                          " estimator", category=UserWarning)

    return estr


def _get_py(pys, px):
    """Algorithm to determine number of y-features to use in parameter sweep.

    Parameters
    ----------
    pys : 'px', function or int
        if 'px' return ``px``, if function return ``function(px)``, if int
        return ``int(pys)``
    px : int
        number of `X` features

    Returns
    -------
    py : int
        number of y-features to use
    """
    if pys == 'px':
        py = px
    else:
        try:  # allow `pys` to be a function
            py = pys(px)
        except TypeError:
            # assume pys is an integer
            try:
                py = int(pys)
            except ValueError:
                raise ValueError('Invalid py: {}'.format(pys))
    return py


def _select_n_per_ftrs(n_per_ftrs, model, ax, ay, r_between):

    if n_per_ftrs != 'auto':
        return n_per_ftrs

    else:

        for _r, _n_per_ftrs in [
            # tuples (r, n_per_ftrs), values were chosen heuristically
            (.1, (3, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192)),
            (.3, (3, 4, 8, 16, 32, 64, 128, 256, 512, 1024)),
            (.5, (3, 4, 8, 16, 32, 64, 128, 256, 512)),
            (.7, (3, 4, 8, 16, 32, 64, 128, 256)),
            (.8, (3, 4, 8, 16, 32, 64, 128)),
            (1., (3, 4, 8, 16, 32, 64)),
        ]:
            if r_between <= _r:
                n_per_ftrs = _n_per_ftrs
                break

        return n_per_ftrs
